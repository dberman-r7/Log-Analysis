# Change Request: CR-2026-02-11-007

## Title
Parquet cache keyed by log + time span with partial-range fetch (multi-parquet segments) and parquet summaries

## Requestor
User (dberman)

## Date
2026-02-11

## Category
FEATURE (Performance / Reliability / Observability)

## Priority
P2-MEDIUM

## Description
Add a local Parquet cache so that when the system downloads a time range from a given log, it persists the events and can reuse them on subsequent runs.

Before calling the Rapid7 API, the service must check the cache for coverage of the requested time window.

**Normative range semantics**: all windows/segments use **inclusive start, exclusive end** `[start_time, end_time)`.

- If cached coverage is complete, do not call the API.
- If cached coverage is partial, compute the exact **missing subranges** of the requested window and fetch **only** those missing subranges.
- **Efficiency correction**: Missing subranges shall be persisted as **additional Parquet segment files** (no in-place merge/compaction required). For analysis, the service shall read from **multiple Parquet files** that collectively cover the requested `(log_id, time window)`.

**Memory constraint requirement**: The current approach may require too much memory. The ingestion pipeline must stream/process Log Search results page-by-page and periodically flush buffered rows to Parquet to free memory. The flush cadence must be configurable (e.g., by max buffered rows / events or pages). Buffer must be cleared after flush.

After processing, output a summary **derived from the cached Parquet dataset** that includes column information and useful aggregates.

## Business Justification
Repeated log-range downloads waste time, consume API quota, and add variability to run duration. A deterministic cache reduces cost and time-to-results, and makes runs more reproducible. A Parquet summary provides fast visibility into the data shape and content without rereading raw API payloads.

## Affected Components
- `src/log_ingestion/service.py` (pipeline orchestration; cache check + partial fetch workflow; streaming flush)
- `src/log_ingestion/parquet_writer.py` (segment writes; dataset reads; summary support)
- `src/log_ingestion/config.py` (cache location, enable/disable flags, flush threshold config)
- `src/log_ingestion/main.py` (CLI output additions for cache + summary)
- New module(s): `src/log_ingestion/cache_index.py` and/or `src/log_ingestion/parquet_summary.py`
- Tests: `tests/test_service.py`, new unit tests for range math, cache index, and streaming flush behavior
- Docs: `docs/requirements/rtm.md`, `docs/runbooks/log-ingestion-service.md`, (optional) `docs/requirements/slos.md`

## Notes / Risks
- Correctness risk in partial-range math (must avoid gaps/overlaps and off-by-one in timestamp boundaries).
- Storage growth + file-count risk; must define cache retention and fail loudly when disk/write errors occur.
- Idempotency risk on rerun: must not double-fetch already-covered ranges; duplicates must be prevented or handled deterministically with explicit logs.
- Must keep structured logs for cache decisions (hit/miss/partial), missing-range fetch plan, streaming flush events, and summary generation.

## Acceptance Criteria
- Cache planner returns **no missing subranges** for fully-covered requests.
- For partial coverage, cache planner returns **non-overlapping missing subranges** that exactly fill the gaps in the requested window (no over-fetch, no under-fetch).
- Streaming ingestion never exceeds the configured buffered-row threshold (except transiently during transformation within a page) and emits flush logs.
- Post-run summary is computed by reading the resulting Parquet dataset for the requested window (multi-segment) and includes: row count, columns/dtypes, and timestamp min/max (if present).

---

## ATP Gate (Governed Change)
This change is **CR-required**. No implementation work (no edits under `src/`, `tests/`, or dependency manifests) may begin until an explicit approval token is provided ("Approved to Proceed" / "ATP" / "Go Ahead" / "Implement").

**Approval Token Received**: ATP
**Approval Date**: 2026-02-12

---

## Impact Assessment (IA)

### Blast Radius / Touchpoints
- **Primary modules**: `src/log_ingestion/service.py`, `src/log_ingestion/parquet_writer.py`, `src/log_ingestion/config.py`, `src/log_ingestion/main.py`
- **New module(s)** likely: cache coverage/index utilities + parquet summary generator
- **Artifacts changed**: on-disk cache directory structure, number/naming of Parquet files, log output, summary output
- **Tests**: service + new unit tests for range math, planner correctness, flush behavior, and multi-parquet reading

### Functional / UX Impact
- **Externally observable behavior changes** (YES):
  - Adds a cache layer that can skip API calls (cache hit) or reduce API calls (partial coverage).
  - Changes output artifacts: multiple Parquet segment files and part files may be produced per log/time window.
  - Adds/changes logs: cache decisions, missing-subrange plan, flush instrumentation, and summary generation.
  - Adds a new “Parquet summary” output section.

### Data / Storage Impact
- New on-disk cache directory structure and file naming conventions.
- Potential growth of Parquet segment files over time; requires retention/cleanup guidance (operator-facing).
- Segment-based approach intentionally avoids rewrite/merge operations; improves write amplification but increases file count.

### Performance & Memory Impact
- Positive: reduces repeated API fetches; reduces time-to-results on subsequent runs.
- Positive: streaming flush reduces peak memory usage by bounding buffered rows.
- Potential negative: many small Parquet files can degrade read performance; mitigated with configurable flush threshold + tuning guidance.

### Reliability / Failure-Mode Impact
- New failure modes:
  - Partial writes / corrupt segments (must be detected and fail loudly; avoid silently using corrupt files).
  - Cache index drift (index says coverage exists but files missing) — must be validated at read time.
  - Off-by-one / boundary errors in coverage math (must be tested; normatively `[start,end)`).
- Idempotency must be explicit: rerunning over the same window should not duplicate segments for already-covered ranges.

### Security / Compliance Impact
- Local persistence of log events increases data-at-rest footprint; must validate cache directory handling and avoid writing outside configured root.
- No secrets may be written to Parquet or logs.

### Observability Impact
- Structured logs for:
  - cache_hit / cache_miss / cache_partial
  - missing_subranges_computed (with from/to) before API calls
  - flush_to_parquet events (rows written, file path)
  - summary_generated (key summary fields)
- Errors must be explicit: cache/index read failures, parquet read/write failures, and unexpected schema changes.

### Test Impact
- Requires new tests for:
  - time-range coverage math (including inclusive/exclusive boundary definition)
  - cache planner for missing subranges (including multiple gaps)
  - streaming flush threshold behavior (writer called multiple times; buffer cleared)
  - dataset read across multiple Parquet segments/parts
  - summary derived from Parquet dataset

### Documentation Impact
- Must update:
  - RTM: new/updated REQs for multi-parquet cache semantics + streaming flush + parquet-derived summary
  - Runbook: cache layout, tuning flush threshold, retention, troubleshooting
  - (Optional) SLOs: cache hit rate, time-to-first-parquet, memory target

### Rollback / Backout Plan
- Disable cache via config flag (bypass cache read/write and revert to API-only behavior).
- Disable streaming flush (set threshold very high or fallback to previous behavior if retained).
- Clear cache directory (operator action) if corruption suspected.

---

## Technical Design

### Goals
- Reuse already-downloaded log events for a `(log_id, time window)` without re-fetching from API.
- If coverage is partial, fetch only the missing subranges.
- Keep memory bounded by streaming pages and periodic Parquet flushes.
- Provide a fast data-shape summary from cached Parquet.

### Non-Goals
- Parquet compaction/merge of existing segments (can be added later if needed).
- Cross-log deduplication.

### Cache Model
- Cache is keyed by `log_id` and time.
- Cache storage is represented as **multiple Parquet segment files**, each with:
  - `log_id`
  - `window_start` / `window_end` metadata (or encoded in filename)
  - `ingestion_run_id` or `fetch_id` for traceability

#### Suggested on-disk layout (subject to implementation plan)
- Root: `${CACHE_DIR}/logs/${log_id}/`
- Segment file(s): `from=<iso8601>/to=<iso8601>/part-00001.parquet` OR a filename-safe encoding.

### Coverage + Missing-Range Planner
- Input: requested `[start_time, end_time)`.
- Discover cached segments intersecting requested window.
- Compute covered union of segments within the requested window.
- Missing subranges = requested window minus covered ranges.
- If missing is empty: cache hit.
- Else: plan API fetch calls for each missing subrange.

### Streaming Ingestion + Flush
- Process Log Search results page by page.
- Transform each page’s events into row dicts.
- Buffer rows up to a configurable threshold, then flush:
  - Write a Parquet segment (or append a “part file” under the segment directory for that missing subrange)
  - Clear buffer
- Configuration:
  - `PARQUET_FLUSH_ROWS` (or pages) with safe default that avoids OOM.

### Multi-Parquet Reads for Analysis
- For any requested window, read all Parquet segments intersecting the window and treat them as one dataset.
- Avoid loading entire dataset if not required; enable column pruning and predicate pushdown where possible.

### Parquet Summary Output
Minimum summary (derived from the dataset read):
- columns present + inferred dtypes
- row count
- min/max timestamps (if present)
- count of nulls per key field (bounded list)
- sample of distinct values for small-cardinality fields (bounded)

### Failure Modes (Fail Loudly)
- Cache index missing/corrupt: log ERROR and either bypass cache or hard-fail (decision captured in implementation plan).
- Parquet write failure: ERROR and abort run (no silent skipping).
- Parquet read failure: ERROR and abort or bypass with explicit warning depending on config.

### Compatibility
- Backwards compatible: cache is additive; API behavior remains available.
- If existing single-output parquet exists, it is treated separately from the cache segments.
